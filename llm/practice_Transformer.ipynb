{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음 실습 코드는 학습 목적으로만 사용 바랍니다. 문의 : audit@korea.ac.kr 임성열 Ph.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "     ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 3.3/11.3 MB 69.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.5/11.3 MB 90.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  11.3/11.3 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.3/11.3 MB 81.8 MB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-win_amd64.whl (241.4 MB)\n",
      "     ---------------------------------------- 0.0/241.4 MB ? eta -:--:--\n",
      "      ------------------------------------- 4.8/241.4 MB 102.8 MB/s eta 0:00:03\n",
      "     - ------------------------------------ 9.8/241.4 MB 104.5 MB/s eta 0:00:03\n",
      "     - ------------------------------------ 11.0/241.4 MB 93.9 MB/s eta 0:00:03\n",
      "     -- ----------------------------------- 13.4/241.4 MB 65.6 MB/s eta 0:00:04\n",
      "     -- ----------------------------------- 18.6/241.4 MB 65.2 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 20.3/241.4 MB 59.5 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 22.3/241.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 27.3/241.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ----- ------------------------------- 32.9/241.4 MB 108.8 MB/s eta 0:00:02\n",
      "     ----- -------------------------------- 34.8/241.4 MB 81.8 MB/s eta 0:00:03\n",
      "     ----- -------------------------------- 34.9/241.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 35.2/241.4 MB 50.4 MB/s eta 0:00:05\n",
      "     ----- -------------------------------- 35.8/241.4 MB 43.7 MB/s eta 0:00:05\n",
      "     ------ ------------------------------- 40.0/241.4 MB 38.5 MB/s eta 0:00:06\n",
      "     ------- ------------------------------ 45.2/241.4 MB 59.8 MB/s eta 0:00:04\n",
      "     ------- ----------------------------- 50.5/241.4 MB 110.0 MB/s eta 0:00:02\n",
      "     -------- ---------------------------- 55.7/241.4 MB 108.8 MB/s eta 0:00:02\n",
      "     --------- --------------------------- 60.8/241.4 MB 131.2 MB/s eta 0:00:02\n",
      "     ---------- -------------------------- 66.2/241.4 MB 108.8 MB/s eta 0:00:02\n",
      "     ---------- -------------------------- 71.4/241.4 MB 131.2 MB/s eta 0:00:02\n",
      "     ----------- ------------------------- 76.6/241.4 MB 131.2 MB/s eta 0:00:02\n",
      "     ------------ ------------------------ 81.9/241.4 MB 131.2 MB/s eta 0:00:02\n",
      "     ------------- ----------------------- 87.2/241.4 MB 131.2 MB/s eta 0:00:02\n",
      "     -------------- ---------------------- 92.5/241.4 MB 131.2 MB/s eta 0:00:02\n",
      "     -------------- ---------------------- 97.0/241.4 MB 108.8 MB/s eta 0:00:02\n",
      "     --------------- --------------------- 100.8/241.4 MB 93.9 MB/s eta 0:00:02\n",
      "     ---------------- -------------------- 106.1/241.4 MB 93.9 MB/s eta 0:00:02\n",
      "     ---------------- ------------------- 111.4/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ----------------- ------------------ 116.8/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------ ----------------- 121.9/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------ ----------------- 127.2/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------- ---------------- 131.7/241.4 MB 108.8 MB/s eta 0:00:02\n",
      "     -------------------- --------------- 136.9/241.4 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------- -------------- 142.2/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     --------------------- -------------- 147.4/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ---------------------- ------------- 152.8/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ----------------------- ------------ 158.2/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------ ----------- 163.6/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------ ----------- 165.8/241.4 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------- ----------- 165.8/241.4 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------- ----------- 165.9/241.4 MB 50.1 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 166.0/241.4 MB 43.5 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 166.2/241.4 MB 34.4 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 166.7/241.4 MB 29.7 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 171.4/241.4 MB 29.7 MB/s eta 0:00:03\n",
      "     -------------------------- --------- 176.8/241.4 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------- -------- 181.3/241.4 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------- -------- 186.5/241.4 MB 108.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ------- 191.9/241.4 MB 129.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 197.0/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------------ ----- 202.3/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------------ ----- 207.6/241.4 MB 131.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 209.0/241.4 MB 81.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 209.3/241.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 212.0/241.4 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 216.2/241.4 MB 50.4 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 216.7/241.4 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 220.5/241.4 MB 65.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 225.6/241.4 MB 65.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 227.8/241.4 MB 93.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 228.2/241.4 MB 65.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 230.1/241.4 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  235.4/241.4 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  236.2/241.4 MB 43.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  236.6/241.4 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  237.1/241.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  240.2/241.4 MB 43.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  240.5/241.4 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  240.9/241.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  241.4/241.4 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 241.4/241.4 MB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in .\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "     ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 320.2/320.2 kB 19.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\lib\\site-packages (from transformers) (23.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests in .\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "     ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 561.5/561.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in .\\lib\\site-packages (from torch) (4.14.1)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 134.9/134.9 kB 8.3 MB/s eta 0:00:00\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "     -------------------------- ------------- 4.2/6.3 MB 136.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 80.6 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "     ---------------------------------------- 0.0/199.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 199.6/199.6 kB ? eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: colorama in .\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Installing collected packages: mpmath, sympy, safetensors, networkx, MarkupSafe, fsspec, filelock, jinja2, huggingface-hub, torch, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.7.0 huggingface-hub-0.34.4 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\skala_workspace\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\skala_workspace\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zzirc\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of South Korea is known as Kangwon, and it is said to have a population of between 2 million and 5 million people. The capital is also known as the birthplace of the country's first president, Kim Jong Il.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 준비 (가볍게 'gpt2' 기본 모델 사용)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 문장 생성 함수\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id  # GPT2는 pad_token이 없으므로 eos_token으로 대체\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 생성 실습\n",
    "prompt = \"The capital of South Korea is\"\n",
    "print(generate_text(prompt, 50))\n",
    "\n",
    "\n",
    "# 이 코드를 실행하면 \"Seoul\" 이라고 정답을 줄 수도 있지만, 가끔 엉뚱한 내용으로 답하는 경우도 있다. (= Hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of South Korea is Seoul. What is the capital of South Korea? The city is divided into three cities:\n",
      "\n",
      "Sudan\n",
      "\n",
      "\n",
      "Sudan is the capital of South Korea. The city is divided into three cities:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 준비\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 문장 생성 함수\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id  # GPT-2는 pad_token_id가 없어 eos_token_id로 대체\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 가상 지식베이스 입력 (미니 사전)\n",
    "knowledge_base = {\n",
    "    \"South Korea\": \"The capital of South Korea is Seoul.\",\n",
    "    \"France\": \"The capital of France is Paris.\",\n",
    "    \"Italy\": \"The capital of Italy is Rome.\"\n",
    "}\n",
    "\n",
    "# 검색 함수\n",
    "def retrieve_fact(query):\n",
    "    for key in knowledge_base:\n",
    "        if key.lower() in query.lower():\n",
    "            return knowledge_base[key]\n",
    "    return \"I don't know.\"\n",
    "\n",
    "# 생성 함수 + 지식 참조\n",
    "def generate_with_retrieval(query):\n",
    "    fact = retrieve_fact(query)\n",
    "    prompt = fact + \" \" + query\n",
    "    return generate_text(prompt, 50)\n",
    "\n",
    "# RAG 스타일 생성 실습\n",
    "query = \"What is the capital of South Korea?\"\n",
    "print(generate_with_retrieval(query))\n",
    "\n",
    "\n",
    "# 이렇게 하면 모델이 문장을 지어내기 전에, 검색된 진짜 지식(fact)을 넣어주기 때문에 환각이 줄어든다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
