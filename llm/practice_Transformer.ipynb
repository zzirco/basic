{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음 실습 코드는 학습 목적으로만 사용 바랍니다. 문의 : audit@korea.ac.kr 임성열 Ph.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: torch in ./llm/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./llm/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./llm/lib/python3.11/site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./llm/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./llm/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./llm/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./llm/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in ./llm/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./llm/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./llm/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./llm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./llm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./llm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./llm/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./llm/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./llm/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./llm/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./llm/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./llm/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./llm/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./llm/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./llm/lib/python3.11/site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Installing collected packages: safetensors, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed safetensors-0.5.3 transformers-4.54.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phoenix/Eagle/2025_교육-KDT/LLM 실습/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of South Korea is a bastion of its prosperity, and its economy is growing at an even faster rate than most of the world's.\n",
      "\n",
      "South Korea has the world's largest economy at over $100 trillion, and it is the\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 준비 (가볍게 'gpt2' 기본 모델 사용)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 문장 생성 함수\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id  # GPT2는 pad_token이 없으므로 eos_token으로 대체\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 생성 실습\n",
    "prompt = \"The capital of South Korea is\"\n",
    "print(generate_text(prompt, 50))\n",
    "\n",
    "\n",
    "# 이 코드를 실행하면 \"Seoul\" 이라고 정답을 줄 수도 있지만, 가끔 엉뚱한 내용으로 답하는 경우도 있다. (= Hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of South Korea is Seoul. What is the capital of South Korea?\n",
      "\n",
      "The capital city of Seoul is located in the South. What is the capital of South Korea?\n",
      "\n",
      "South Korea is home to a large number of different cultural\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 준비\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 문장 생성 함수\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id  # GPT-2는 pad_token_id가 없어 eos_token_id로 대체\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 가상 지식베이스 입력 (미니 사전)\n",
    "knowledge_base = {\n",
    "    \"South Korea\": \"The capital of South Korea is Seoul.\",\n",
    "    \"France\": \"The capital of France is Paris.\",\n",
    "    \"Italy\": \"The capital of Italy is Rome.\"\n",
    "}\n",
    "\n",
    "# 검색 함수\n",
    "def retrieve_fact(query):\n",
    "    for key in knowledge_base:\n",
    "        if key.lower() in query.lower():\n",
    "            return knowledge_base[key]\n",
    "    return \"I don't know.\"\n",
    "\n",
    "# 생성 함수 + 지식 참조\n",
    "def generate_with_retrieval(query):\n",
    "    fact = retrieve_fact(query)\n",
    "    prompt = fact + \" \" + query\n",
    "    return generate_text(prompt, 50)\n",
    "\n",
    "# RAG 스타일 생성 실습\n",
    "query = \"What is the capital of South Korea?\"\n",
    "print(generate_with_retrieval(query))\n",
    "\n",
    "\n",
    "# 이렇게 하면 모델이 문장을 지어내기 전에, 검색된 진짜 지식(fact)을 넣어주기 때문에 환각이 줄어든다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
